{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQnqWg5hzKCE"
      },
      "source": [
        "import sys\r\n",
        "import nltk\r\n",
        "import sklearn\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPAzNQ9nzW3m",
        "outputId": "84e2e16f-353e-45a4-8b02-0852e010ffaf"
      },
      "source": [
        "# data_frame= pd.read_table(\"SMSSpamCollection\",header= None, encoding ='utf-8') \r\n",
        "# print(data_frame.head())\r\n",
        "\r\n",
        "import nltk\r\n",
        "# nltk.download('stopwords')\r\n",
        "\r\n",
        "# nltk.download('wordnet')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvFfqUe5zbpl",
        "outputId": "20e616a5-f09b-4106-ae5b-354609be4d2c"
      },
      "source": [
        "#Preprocessing\r\n",
        "from sklearn import preprocessing\r\n",
        "le = preprocessing.LabelEncoder()\r\n",
        "classes= le.fit_transform(data_frame[0])\r\n",
        "\r\n",
        "#replacing email, phonr No, money, webaddress, number, puntuation, more than single space, leading and trailing white space\r\n",
        "emails= data_frame[1]\r\n",
        "\r\n",
        "emails= emails.str.replace(r'^\\w+@[a-zA-Z_]+?\\.[a-zA-Z]{2,3}$', 'emailaddress')\r\n",
        "emails= emails.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', 'phonenumber')\r\n",
        "emails= emails.str.replace(r'^£|\\$', 'money')\r\n",
        "emails= emails.str.replace(r'\\d+(\\.\\d+)?','number')\r\n",
        "emails= emails.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','webaddress')\r\n",
        "emails= emails.str.replace(r'[^\\w\\d\\s]', ' ')\r\n",
        "emails= emails.str.replace(r'\\s+', ' ')\r\n",
        "emails= emails.str.replace(r'^\\s+|\\s+?$', '')\r\n",
        "emails= emails.str.lower()\r\n",
        "\r\n",
        "# remove stop words from text messages\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop_words= list(stopwords.words('english'))\r\n",
        "emails=emails.apply(lambda x: ' '.join(i for i in x.split() if i not in stop_words))\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "emails=emails.apply(lambda x: ' '.join(lemmatizer.lemmatize(i) for i in x.split()))\r\n",
        "\r\n",
        "print(emails)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       go jurong point crazy available bugis n great ...\n",
            "1                                 ok lar joking wif u oni\n",
            "2       free entry number wkly comp win fa cup final t...\n",
            "3                     u dun say early hor u c already say\n",
            "4                     nah think go usf life around though\n",
            "                              ...                        \n",
            "5567    numbernd time tried number contact u u number ...\n",
            "5568                          ü b going esplanade fr home\n",
            "5569                                 pity mood suggestion\n",
            "5570    guy bitching acted like interested buying some...\n",
            "5571                                       rofl true name\n",
            "Name: 1, Length: 5572, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3CGk5c911wz"
      },
      "source": [
        "\r\n",
        "new=list()\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "for i in emails:\r\n",
        "    words= word_tokenize(i)\r\n",
        "    for k in words: new.append(k)\r\n",
        "freq_words = nltk.FreqDist( new)\r\n",
        "freq_words.most_common() \r\n",
        "feature_words=list(freq_words.keys())[:1500]\r\n",
        "def features(msg):\r\n",
        "    words= word_tokenize(msg)\r\n",
        "    features= {}\r\n",
        "    for i in feature_words:\r\n",
        "        features[i]= (i in words)\r\n",
        "    return features\r\n",
        "data_set= zip(emails, classes)\r\n",
        "feature_msgs=[(features(emails), y) for(emails,y) in data_set]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OmMfz7c2M5k",
        "outputId": "ec2db9a7-f1ae-436b-da08-03084109564e"
      },
      "source": [
        "from sklearn import model_selection\r\n",
        "\r\n",
        "# split the data into training and testing datasets\r\n",
        "training, testing = model_selection.train_test_split(feature_msgs, test_size = 0.30)\r\n",
        "print(len(training))\r\n",
        "print(len(testing))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3900\n",
            "1672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T0-PxOj2XGc",
        "outputId": "b61ab74f-4b27-4304-e21a-d3693434d766"
      },
      "source": [
        "from nltk.classify.scikitlearn import SklearnClassifier\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "model = SklearnClassifier(MultinomialNB()).train(training)\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\r\n",
        "test_features, labels = zip(*testing)\r\n",
        "predictions= model.classify_many(test_features)\r\n",
        "print(classification_report(labels, predictions))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1441\n",
            "           1       0.94      0.95      0.95       231\n",
            "\n",
            "    accuracy                           0.99      1672\n",
            "   macro avg       0.97      0.97      0.97      1672\n",
            "weighted avg       0.99      0.99      0.99      1672\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKcNJ0lz2Y_M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}